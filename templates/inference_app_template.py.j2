from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from google.cloud import storage
import pandas as pd
import pickle
import io

app = FastAPI(title="Prophet Inference Server")

# Render-time constants
BUCKET_NAME = "{{ bucket_name }}"
MODEL_BLOB_NAME = "{{ model_blob_name }}"
TIMESTAMP_COLUMN = "{{ timestamp_column }}"
OUTPUT_COLUMN = "{{ output_column }}"
INPUT_COLUMNS = {{ input_columns | tojson }}

_model = None
_storage = storage.Client()

def _load_model():
    global _model
    if _model is not None:
        return _model
    bucket = _storage.bucket(BUCKET_NAME)
    blob = bucket.blob(MODEL_BLOB_NAME)
    data = blob.download_as_bytes()
    _model = pickle.loads(data)
    return _model

@app.get("/")
def health():
    return {"status": "ok"}

@app.post("/predict")
async def predict(request: Request):
    try:
        payload = await request.json()
        instances = payload.get("instances")
        if not instances:
            return JSONResponse({"error": "`instances` array required for prediction"}, status_code=400)

        df = pd.DataFrame(instances)
        if df.empty:
            return {"predictions": [], "message": "No instances provided"}

        if TIMESTAMP_COLUMN not in df.columns:
            return JSONResponse({"error": f"`{TIMESTAMP_COLUMN}` field missing in instances"}, status_code=400)

        # Ensure all expected regressors are present
        regressors = INPUT_COLUMNS or []
        missing_regressors = [col for col in regressors if col not in df.columns]
        if missing_regressors:
            return JSONResponse({"error": f"Missing regressor fields: {missing_regressors}"}, status_code=400)

        original_payload_columns = [col for col in df.columns if col not in regressors + [TIMESTAMP_COLUMN]]

        df = df.rename(columns={TIMESTAMP_COLUMN: "ds"})
        df["ds"] = pd.to_datetime(df["ds"])

        model = _load_model()
        pred_df = df[["ds"] + regressors] if regressors else df[["ds"]]
        forecast = model.predict(pred_df)

        # Return only essential columns
        result = forecast[["ds", "yhat", "yhat_lower", "yhat_upper"]].copy()
        
        # Calculate anomaly flag if output_column (actual value) is present
        if OUTPUT_COLUMN in df.columns:
            actual_values = df[OUTPUT_COLUMN].reset_index(drop=True)
            # Anomaly: actual value is outside the prediction interval
            result["is_anomaly"] = (actual_values < result["yhat_lower"]) | (actual_values > result["yhat_upper"])
        else:
            # If no actual values, set is_anomaly to False
            result["is_anomaly"] = False
        
        result["ds"] = result["ds"].apply(lambda x: x.isoformat() if pd.notna(x) else None)

        # Merge back any non-feature columns (e.g., identifiers) from original payload
        if original_payload_columns:
            result[original_payload_columns] = df[original_payload_columns].reset_index(drop=True)

        return {"predictions": result.to_dict(orient="records")}

    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)


